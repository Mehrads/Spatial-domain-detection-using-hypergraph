{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soltani8/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import os,csv,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import issparse\n",
    "import random, torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.colors as clr\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(\"sample_data.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = pd.read_csv(\"adj.csv\")\n",
    "\n",
    "# Calculate the node degree (sum of connections per node)\n",
    "node_degree = np.sum(adj_matrix, axis=1)\n",
    "\n",
    "# Calculate the mean connection strength\n",
    "mean_connection_strength = np.mean(adj_matrix, axis=1)\n",
    "\n",
    "\n",
    "# Add these features to the obs (observations) of AnnData\n",
    "adata.obs['node_degree'] = node_degree\n",
    "adata.obs['mean_connection_strength'] = mean_connection_strength\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the newly added features\n",
    "adata.obs['node_degree_scaled'] = scaler.fit_transform(adata.obs[['node_degree']])\n",
    "adata.obs['mean_connection_strength_scaled'] = scaler.fit_transform(adata.obs[['mean_connection_strength']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.open(\"histology.tif\")\n",
    "img_array = np.array(img)\n",
    "\n",
    "s = 1\n",
    "b = 49\n",
    "\n",
    "# Function to extract color using coordinates, now includes averaging over an area\n",
    "def extract_color(x_pixels, y_pixels, image_array, beta, area_size=10):\n",
    "    colors = []\n",
    "    half_area = area_size // 2\n",
    "    for x, y in zip(x_pixels, y_pixels):\n",
    "        # Define area bounds\n",
    "        x_min = max(x - half_area, 0)\n",
    "        y_min = max(y - half_area, 0)\n",
    "        x_max = min(x + half_area, image_array.shape[1])\n",
    "        y_max = min(y + half_area, image_array.shape[0])\n",
    "        # Extract and average color in the area\n",
    "        area = image_array[y_min:y_max, x_min:x_max]\n",
    "        area_average = np.mean(area, axis=(0, 1))\n",
    "        adjusted_color = area_average * beta / 255\n",
    "        colors.append(np.mean(adjusted_color))\n",
    "    return np.array(colors)\n",
    "\n",
    "# Apply color extraction\n",
    "x_pixel = np.array(adata.obs[\"x4\"].tolist())\n",
    "y_pixel = np.array(adata.obs[\"x5\"].tolist())\n",
    "adata.obs[\"color\"] = extract_color(x_pixel, y_pixel, img_array, b)\n",
    "\n",
    "z_scale = np.std(adata.obs[\"color\"]) * s\n",
    "\n",
    "# Apply normalization and scaling to the 'color' attribute to derive 'z'\n",
    "adata.obs[\"z\"] = (adata.obs[\"color\"] - np.mean(adata.obs[\"color\"])) / np.std(adata.obs[\"color\"]) * z_scale\n",
    "\n",
    "# Clean up by deleting loaded images if no longer needed\n",
    "del img, img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "adata.obs[\"x_pixel\"]=x_pixel\n",
    "adata.obs[\"y_pixel\"]=y_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var_names_make_unique()\n",
    "sc.pp.normalize_per_cell(adata, min_counts=0)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3639 × 33538\n",
       "    obs: 'x1', 'x2', 'x3', 'x4', 'x5', 'x_array', 'y_array', 'x_pixel', 'y_pixel', 'node_degree', 'mean_connection_strength', 'node_degree_scaled', 'mean_connection_strength_scaled', 'color', 'z', 'n_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'genename'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dhg import Hypergraph\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # X is your feature matrix derived from spatial coordinates and any additional features\n",
    "# X = np.array([adata.obs[\"x_pixel\"], adata.obs[\"y_pixel\"], adata.obs[\"z\"]]).T.astype(np.float32)\n",
    "# nbrs = NearestNeighbors(n_neighbors=10, n_jobs=-1).fit(X)  # 10 or 5\n",
    "# edges = nbrs.kneighbors_graph(X, mode='connectivity').toarray()\n",
    "\n",
    "# # Convert the adjacency matrix to list of hyperedges\n",
    "# hyperedges = [list(np.where(row)[0]) for row in edges if np.any(row)]\n",
    "\n",
    "# # Number of vertices is the number of rows in your data\n",
    "# num_vertices = X.shape[0]\n",
    "\n",
    "# # Create the Hypergraph using the hyperedges\n",
    "# G = Hypergraph(num_vertices, hyperedges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from dhg import Hypergraph\n",
    "\n",
    "# Load and preprocess data\n",
    "selected_features = [\"x_pixel\", \"y_pixel\", \"z\", \"color\", \"node_degree_scaled\", \"mean_connection_strength_scaled\"]  # Spatial coordinates and additional features\n",
    "X = np.array([adata.obs[f] for f in selected_features]).T.astype(np.float32)\n",
    "\n",
    "# Example: Fill NaNs with the mean of each column\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Now scale the data\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "\n",
    "# Constructing a spatial graph based on Euclidean distances\n",
    "nbrs = NearestNeighbors(n_neighbors=15, metric='euclidean', n_jobs=-1).fit(X_scaled)\n",
    "edges = nbrs.kneighbors_graph(X_scaled, mode='connectivity').toarray()\n",
    "\n",
    "def create_weighted_hyperedges(edges):\n",
    "    \"\"\"\n",
    "    Create hyperedges with weights based on the inverse of Euclidean distances,\n",
    "    simulating a stronger connection between closer nodes.\n",
    "    \"\"\"\n",
    "    hyperedges = []\n",
    "    weights = []\n",
    "    for i in range(edges.shape[0]):\n",
    "        if np.any(edges[i]):\n",
    "            center = X_scaled[i]\n",
    "            neighbors = np.where(edges[i])[0]\n",
    "            distances = np.linalg.norm(X_scaled[neighbors] - center, axis=1)\n",
    "            weights.append(1 / (1 + distances))  # Inverse distance, avoiding division by zero\n",
    "            hyperedges.append(list(neighbors))\n",
    "    return hyperedges, weights\n",
    "\n",
    "# Generate hyperedges with corresponding weights\n",
    "hyperedges, weights = create_weighted_hyperedges(edges)\n",
    "\n",
    "# Initialize the hypergraph\n",
    "num_vertices = X_scaled.shape[0]\n",
    "G = Hypergraph(num_vertices, hyperedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from typing import List, Tuple  # For type hinting\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to select features based on type (optional)\n",
    "# def get_features_by_type(adata: AnnData, feature_type: str) -> List[str]:\n",
    "#   \"\"\"\n",
    "#   This function retrieves features from obs based on their type in var.\n",
    "\n",
    "#   Args:\n",
    "#       adata: An AnnData object.\n",
    "#       feature_type: The desired feature type (e.g., \"continuous\", \"categorical\").\n",
    "\n",
    "#   Returns:\n",
    "#       A list of feature names that match the specified type.\n",
    "#   \"\"\"\n",
    "#   return [f for f in adata.obs.keys() if adata.var[\"feature_types\"][f] == feature_type]\n",
    "\n",
    "# # Feature selection (replace with your desired features)\n",
    "# selected_features = [\"x_pixel\", \"y_pixel\", \"z\", \"color\", \"node_degree_scaled\", \"mean_connection_strength_scaled\"]\n",
    "\n",
    "# # Feature pre-processing (consider feature type and normalization techniques)\n",
    "# X = np.array([adata.obs[f] for f in selected_features]).T.astype(np.float32)\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# # Now scale the data\n",
    "# X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# # Hypergraph construction using NetworkX for efficiency\n",
    "# def create_hypergraph(X_scaled: np.ndarray, n_neighbors: int) -> Tuple[nx.Graph, List[List[int]], List[float]]:\n",
    "#   \"\"\"\n",
    "#   This function constructs a hypergraph using NetworkX.\n",
    "\n",
    "#   Args:\n",
    "#       X_scaled: The preprocessed feature matrix.\n",
    "#       n_neighbors: The number of nearest neighbors for each data point.\n",
    "\n",
    "#   Returns:\n",
    "#       A tuple containing the NetworkX graph object, a list of hyperedges, and a list of weights.\n",
    "#   \"\"\"\n",
    "#   G = nx.Graph()\n",
    "#   G.add_nodes_from(range(X_scaled.shape[0]))  # Add nodes (data points)\n",
    "\n",
    "#   # Create hyperedges with optional weighting\n",
    "#   hyperedges = []\n",
    "#   weights = []\n",
    "#   for i in range(X_scaled.shape[0]):\n",
    "#     neighbors = nbrs.kneighbors(X_scaled[i].reshape(1, -1))[1][0]  # Get nearest neighbors\n",
    "#     if np.any(neighbors):\n",
    "#       center_index = i\n",
    "#       neighbor_list = neighbors.tolist()\n",
    "#       G.add_edge(center_index, *neighbor_list)  # Add hyperedge using unpacking\n",
    "#       # Optional weighting (consider alternative weighting functions)\n",
    "#       distances = np.linalg.norm(X_scaled[neighbors] - X_scaled[center_index], axis=1)\n",
    "#       weights.append(1 / (1 + distances))  # Inverse distance weighting\n",
    "\n",
    "#   return G, hyperedges, weights\n",
    "\n",
    "# # Construct the hypergraph\n",
    "# nbrs = NearestNeighbors(n_neighbors=15, metric='euclidean', n_jobs=-1).fit(X_scaled)\n",
    "# G, hyperedges, weights = create_hypergraph(X_scaled, n_neighbors=15)\n",
    "\n",
    "# # Hypergraph analysis using NetworkX or other libraries\n",
    "# # ... (Your analysis code here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the sparse matrix to a dense array first\n",
    "dense_features = adata.X.toarray()  # Converts the sparse matrix to a dense numpy array\n",
    "features = torch.tensor(dense_features, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dhg.models import HGNNP\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HGNN_Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HGNN_Autoencoder, self).__init__()\n",
    "        # Define encoder using nn.ModuleDict\n",
    "        self.encoder = nn.ModuleDict({\n",
    "            'hgnnp1': HGNNP(input_dim, hidden_dim, hidden_dim, use_bn=True),\n",
    "            'relu': nn.ReLU(),\n",
    "            'dropout': nn.Dropout(0.5),\n",
    "            'hgnnp2': HGNNP(hidden_dim, hidden_dim, output_dim, use_bn=True)\n",
    "        })\n",
    "        # Define decoder using nn.Sequential\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, G):\n",
    "        x = self.encode(x, G)  # Use the new encode method\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x, G):\n",
    "        # Explicitly define the forwarding through encoder layers\n",
    "        for layer in ['hgnnp1', 'relu', 'dropout', 'hgnnp2']:\n",
    "            if 'hgnnp' in layer:\n",
    "                x = self.encoder[layer](x, G)\n",
    "            else:\n",
    "                x = self.encoder[layer](x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Parameters\n",
    "input_dim = features.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 32  # Could be tuned based on the complexity of the data\n",
    "\n",
    "# Initialize the model\n",
    "model = HGNN_Autoencoder(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) # 0.1 is close to the spaGCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1048\n",
      "Epoch 10, Loss: 0.0595\n",
      "Epoch 20, Loss: 0.0563\n",
      "Epoch 30, Loss: 0.0553\n",
      "Epoch 40, Loss: 0.0549\n",
      "Epoch 50, Loss: 0.0550\n",
      "Epoch 60, Loss: 0.0550\n",
      "Epoch 70, Loss: 0.0549\n",
      "Epoch 80, Loss: 0.0548\n",
      "Epoch 90, Loss: 0.0547\n",
      "Epoch 100, Loss: 0.0545\n",
      "Epoch 110, Loss: 0.0540\n",
      "Epoch 120, Loss: 0.0510\n",
      "Epoch 130, Loss: 0.0375\n",
      "Epoch 140, Loss: 0.0355\n",
      "Epoch 150, Loss: 0.0346\n",
      "Epoch 160, Loss: 0.0344\n",
      "Epoch 170, Loss: 0.0340\n",
      "Epoch 180, Loss: 0.0335\n",
      "Epoch 190, Loss: 0.0333\n"
     ]
    }
   ],
   "source": [
    "def train(model, features, G, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features, G)\n",
    "        loss = F.mse_loss(outputs, features)  # Mean Squared Error loss for reconstruction\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "train(model, features, G, optimizer, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    encoded_features = model.encode(features, G)  # Get the encoded features using the new method\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)  # Adjust the number of clusters as needed\n",
    "cluster_labels = kmeans.fit_predict(encoded_features.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score:  0.56419754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "labels = kmeans.labels_\n",
    "score = silhouette_score(encoded_features, cluster_labels)\n",
    "print(\"Silhouette Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_use = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#bcbd22', '#17becf', \n",
    "              '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#bec1d4', '#bb7784', '#0000ff', '#111010', '#FFFF00', \n",
    "              '#1f77b4', '#800080', '#959595', '#7d87b9', '#bec1d4', '#d6bcc0', '#bb7784', '#8e063b', '#4a6fe3', \n",
    "              '#8595e1', '#b5bbe3', '#e6afb9', '#e07b91', '#d33f6a', '#11c638', '#8dd593', '#c6dec7', '#ead3c6', \n",
    "              '#f0b98d', '#ef9708', '#0fcfc0', '#9cded6', '#d5eae7', '#f3e1eb', '#f6c4e1', '#f79cd4']\n",
    "# Visualization or further analysis can be done using `cluster_labels`\n",
    "adata.obs[\"pred\"] = pd.Categorical(cluster_labels)\n",
    "num_celltype = len(adata.obs['pred'].cat.categories)\n",
    "adata.uns['pred_colors'] = colors_use[:num_celltype]\n",
    "\n",
    "# Plotting\n",
    "ax = sc.pl.scatter(adata, alpha=1, x='y_pixel', y='x_pixel', color='pred', title='Spatial Clustering',\n",
    "                   palette=colors_use[:num_celltype], show=False, size=150000 / adata.shape[0])  # Adjust size as needed\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.axes.invert_yaxis()\n",
    "\n",
    "# Saving the plot\n",
    "plt.savefig(\"results/Another/multi_sections_domains_adjacency_2.png\", dpi=600)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
